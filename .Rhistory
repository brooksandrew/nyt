ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') paste0(i, ' - could not extract article')
Sys.sleep(.5)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(artxt, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
########################################################
## this function generates the URL for a GET request to the New York Times Article Search API
########################################################
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
## examples
if(1==0){
library('httr')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20131025', key='sample-key')
}
########################################################
## this function actually makes the GET requests to the NYT Article Search API
########################################################
getMeta <- function(url, pages=Inf, sleep=0.1) {
art <- list()
i <- 1
e <- c(-3,-2,-1)
while(i<pages){
if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1
tryget <- try({
urlp <- gsub('page=\\d+', paste0('page=', i), url)
p <- GET(urlp)
pt <- content(p, 'parsed')
if(length(pt$response$docs)>0) art <- append(art, pt$response$docs)
else {print(paste0(i, ' pages collected')); break}
print(i)
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=111, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F) {
metaArt <- meta
if(overwrite==T) {artIndex <- 1:n
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') paste0(i, ' - could not extract article')
Sys.sleep(.5)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(artxt, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
########################################################
## this function generates the URL for a GET request to the New York Times Article Search API
########################################################
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
## examples
if(1==0){
library('httr')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20131025', key='sample-key')
}
########################################################
## this function actually makes the GET requests to the NYT Article Search API
########################################################
getMeta <- function(url, pages=Inf, sleep=0.1) {
art <- list()
i <- 1
e <- c(-3,-2,-1)
while(i<pages){
if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1
tryget <- try({
urlp <- gsub('page=\\d+', paste0('page=', i), url)
p <- GET(urlp)
pt <- content(p, 'parsed')
if(length(pt$response$docs)>0) art <- append(art, pt$response$docs)
else {print(paste0(i, ' pages collected')); break}
print(i)
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=111, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F) {
metaArt <- meta
if(overwrite==T) {artIndex <- 1:n
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') paste0(i, ' - could not extract article')
Sys.sleep(.5)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
########################################################
## this function generates the URL for a GET request to the New York Times Article Search API
########################################################
makeURL <- function(q=NULL, fq=NULL, begin_date=NULL, end_date=NULL, key=getOption("nyt_as_key"), page=0){
arglist <- list(q=q, fq=fq, begin_date=begin_date, end_date=end_date, 'api-key'=key, page=page)
url <- 'http://api.nytimes.com/svc/search/v2/articlesearch.json?'
for(i in 1:length(arglist)){
if(is.null(unlist(arglist[i]))==F){
url <- paste0(url, '&', names(arglist[i]), '=', arglist[i])
}
}
return(url)
}
## examples
if(1==0){
library('httr')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20131025', key='sample-key')
}
########################################################
## this function actually makes the GET requests to the NYT Article Search API
########################################################
getMeta <- function(url, pages=Inf, sleep=0.1) {
art <- list()
i <- 1
e <- c(-3,-2,-1)
while(i<pages){
if(length(unique(e[(length(e)-2):length(e)]))==1) i <- i+1
tryget <- try({
urlp <- gsub('page=\\d+', paste0('page=', i), url)
p <- GET(urlp)
pt <- content(p, 'parsed')
if(length(pt$response$docs)>0) art <- append(art, pt$response$docs)
else {print(paste0(i, ' pages collected')); break}
print(i)
i <- i+1
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(art)
}
## example
if(1==0){
library('httr')
url <- makeURL(q='andrew', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=111, sleep=0)
}
########################################################
## this function extracts the text for a list of URLs
########################################################
getArticles <- function(meta, n=Inf, overwrite=F, sleep=0.1) {
metaArt <- meta
if(overwrite==T) {artIndex <- 1:n
} else {
ii <-  which(sapply(meta, function(x) is.null(x[['bodyHTML']])))
artIndex <- ii[1:min(n,length(ii))]
}
for(i in artIndex){
tryget <- try({
if(overwrite==T | (is.null(meta[[i]]$body)==T & overwrite==F)) {
p <- GET(meta[[i]]$web_url)
metaArt[[i]]$bodyHTML <- content(p, 'text')
metaArt[[i]]$body <- parseArticleBody(metaArt[[i]]$bodyHTML)
print(i)
}
})
if(class(tryget)=='try-error') {
print(paste0(i, ' error - not scraped'))
e <- c(e, i)
e <- e[(length(e)-2):length(e)]
Sys.sleep(0.5) ## probably scraping too fast -- slowing down
}
Sys.sleep(sleep)
}
return(metaArt)
}
## example
if(1==0){
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
}
########################################################
## This fucnction strips out just the text of an article
########################################################
parseArticleBody <- function(artHTML) {
xpath2try <- c('//div[@class="articleBody"]//p',
'//p[@class="story-body-text story-content"]',
'//p[@class="story-body-text"]'
)
for(xp in xpath2try) {
bodyi <- paste(xpathSApply(htmlParse(artHTML), xp, xmlValue), collapse='')
if(nchar(bodyi)>0) break
}
return(bodyi)
}
## example
if(1==0) {
bb <- parseArticleBody(artxt)
write.csv(bb[[1]], file='art1.csv')
}
library('httr')
library('XML')
url <- makeURL(q='andrew+brooks', begin_date='20110101', end_date='20111025', key='sample-key')
a <- getMeta(url, pages=25)
artxt <- getArticles(a, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
artxt <- getArticles(artxt, n=12, overwrite=F)
library('httr')
library('XML')
## make urls to get meta data
url <- makeURL(begin_date='20140102', end_date='20140105', key='sample-key') #, key='sample-key'
## collect meta data
a <- getMeta(url, pages=15)
## collect articles
artxt <- getArticles(a, n=20, overwrite=F)
artxt <- getArticles(artxt, n=20, overwrite=F, sleep=0)
artxt <- getArticles(artxt, n=15, overwrite=F, sleep=0)
sqrt/9238
sqrt(238)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
dbShowCollections(mg1)
query <- dbGetQuery(mg1, 'testData', "{x:3}")
## insert document
a[[2]]$body <- bb[[2]]
doc <- a[[2]]
output <- dbInsertDocument(mg1, "nyt", toJSON(doc))
query <- dbGetQuery(mg1, 'nyt', "{wordcount:128}")
query
mg1 <- mongoDbConnect('nyt')
mg2 <- mongoDbConnect('nyt')
mg2
dbShowCollections(mg2)
dbShowCollections
dbShowCollections(mg2)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
dbShowCollections(mg1)
library('RMongo')
library('rjson')
mg1 <- mongoDbConnect('nyt')
mongoDbConnect(host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='s063240.mongolab.com:63240')
mongoDbConnect(dbName='nyt', host='63240')
mg1 <- mongoDbConnect(dbName='nyt', host='63240')
dbShowCollections(mg1)
host <- "ds063240.mongolab.com:63240/"
username <- "ajb073"
password <- "andrewb03"
db <- "nyt"
mongo <- mongo.create(host=host , db=db, username=username, password=password)
library('rmongodb')
mongo <- mongo.create(host=host , db=db, username=username, password=password)
setwd('/Users/abrooks/Documents/github/nyt')
source('getArtURL.R')
## ########################
## PARAMETERS
## ########################
days <- gsub('-', '', seq(Sys.Date()-365, Sys.Date()-1, by=1))
mongoCreds <- list(dbName='nyt', collection='articles1', host='ds063240.mongolab.com:63240', username='user1', password='password')
key <- 'sample-key' #scan('key.txt', what='character')
3000/12
500/12
41*300
41*3000
500/33
15*13
library('RMongo')
mongo <- list(dbName='nyt', collection='articles1', host='ds063240.mongolab.com:63240', username='user1', password='password')
con <- mongoDbConnect(mongo$dbName, mongo$host)
authenticated <-dbAuthenticate(con, username=mongo$username, password=mongo$password)
output <- dbGetQuery(con, 'articles1', '{}', 0, 10)
print(output)
class(output)
dim(output)
View(output)
names(output)
output[1,'body']
output[2,'body']
output[3,'body']
output[4,'body']
output[5,'body']
output[6,'body']
output[7,'body']
output <- dbGetQuery(con, 'articles1', '{}', 0, 100)
output <- dbGetQuery(con, 'articles1', '{}', 0, 1000)
output <- dbGetQuery(con, 'articles1', '{}', 0, 10000)
sum(nchar(output$body)==0)
sum(nchar(output$body)>0)
hist(nchar(output$body)>0)
hist(nchar(output$body))
output <- dbGetQuery(con, 'articles1', '{"source":"AP"}', 0, 10000)
hist(nchar(output$body))
length(output)
dim(output)
gc()
View(output)
View(output$body[1:100])
View(output$body[1:1000])
View(output$body[9000:10000])
sum(nchar(output$body)>50)
output <- dbGetQuery(con, 'articles1', '{"source":"New York Times"}')
dim(output)
output <- dbGetQuery(con, 'articles1', '{}', 0, 1000)
dim(output)
table(output$source)
names(output)
unique(output$source)
output <- dbGetQuery(con, 'articles1', '{"source":"The New York Times"}', 0, 1000)
dim(output)
hist(nchar(output$body))
table(nchar(output$body))
sort(table(nchar(output$body)))
sort(table(nchar(output$body)>50))
View(output$body[1:100])
View(output$body[1:1000])
View(output[1:1000, c('body', 'url')])
View(output[1:1000, c('body', 'web_url')])
View(output[1:1000, c('web_url', 'body')])
View(output[1:1000,])
output[58,]
output[59,]
View(output[59,])
View(output[60,])
?dbGetQuery
output <- dbGetQuery(con, 'articles1', '{"source":"The New York Times"}', 0, 1000)
dim(output)
output <- dbGetQuery(con, 'articles1', '{"source":"The New York Times"}', 0, 500)
dim(output)
sum(output$X_id!='')
sum(output$X_id!='' | is.na(output$X_id))
which(output$X_id!='' | is.na(output$X_id))
output$web_url[which(output$X_id!='' | is.na(output$X_id))]
output$document_type[which(output$X_id!='' | is.na(output$X_id))]
table(output$document_type[which(output$X_id!='' | is.na(output$X_id))])
sort(table(output$document_type[which(output$X_id!='' | is.na(output$X_id))]))
library('rmongodb')
?mongo.find.one
